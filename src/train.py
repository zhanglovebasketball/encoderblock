import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from model import TransformerEncoder
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import random, numpy as np, os, requests
from utils.config import load_config


def prepare_datasets_by_sequences(data, train_ratio=0.8, val_ratio=0.1,seed=42):
    """æŒ‰åºåˆ—åˆ†å‰²ï¼Œé¿å…åˆ‡æ–­ä¸Šä¸‹æ–‡"""
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    random.seed(seed)
    # å…ˆæ‰“ä¹±æ•°æ®é¡ºåº
    shuffled_data = data.copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
    random.shuffle(shuffled_data)
    total_sequences = len(shuffled_data)
    train_size = int(total_sequences * train_ratio)
    val_size = int(total_sequences * val_ratio)

    train_data = shuffled_data[:train_size]
    val_data = shuffled_data[train_size:train_size + val_size]
    test_data = shuffled_data[train_size + val_size:]

    return train_data, val_data, test_data

def evaluate_model(model, loader, criterion, device):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))
            total_loss += loss.item()
    return total_loss / len(loader)
def set_seed(seed=42):
    random.seed(seed)#è®¾ç½®Pythonå†…ç½®éšæœºæ•°ç”Ÿæˆå™¨
    np.random.seed(seed)#è®¾ç½®NumPyéšæœºæ•°ç”Ÿæˆå™¨
    torch.manual_seed(seed)#è®¾ç½®PyTorch CPUéšæœºç§å­
    torch.cuda.manual_seed_all(seed)#è®¾ç½®PyTorch GPUéšæœºç§å­

def get_tiny_shakespeare():
    os.makedirs("../data", exist_ok=True)
    path = "../data/tiny_shakespeare.txt"
    if not os.path.exists(path):
        print("ğŸ“¥ ä¸‹è½½ tiny_shakespeare.txt ...")
        url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        r = requests.get(url)
        with open(path, "w", encoding="utf-8") as f:
            f.write(r.text)
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    # æˆªå–ä¸€å°éƒ¨åˆ†ï¼Œè®­ç»ƒå¿«
    return text[:400000]


def plot_losses(train_losses, val_losses, save_path="../results/train_val_curve.png"):
    """ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿"""
    plt.figure(figsize=(10, 6))

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)

    # è®¾ç½®å›¾è¡¨å±æ€§
    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    # ä¿å­˜å›¾åƒ
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
def main():
    # åŠ è½½é…ç½®
    cfg = load_config()
    batch_size = cfg['batch_size']
    learning_rate = cfg['learning_rate']
    epochs = cfg['epochs']
    set_seed(42)
    text = get_tiny_shakespeare()

    from transformers import GPT2TokenizerFast
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

    # æŠŠæ–‡æœ¬åˆ‡å—æˆå°å¥å­
    def split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32):
        """æŒ‰tokenè¾¹ç•Œåˆ‡å—ï¼Œé¿å…åˆ‡æ–­å•è¯"""
        # å…ˆtokenizeæ•´ä¸ªæ–‡æœ¬
        tokens = tokenizer.encode(text)

        chunks = []
        start = 0
        while start < len(tokens):
            # å–ä¸€ä¸ªå—
            end = min(start + max_tokens, len(tokens))
            chunk_tokens = tokens[start:end]

            # è§£ç å›æ–‡æœ¬ï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
            chunk_text = tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)

            # æ»‘åŠ¨çª—å£ï¼Œä½¿ç”¨é‡å 
            start += (max_tokens - overlap)

        return chunks

    # ä½¿ç”¨æ”¹è¿›çš„åˆ‡å—
    samples = split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32)
    #åˆ†è¯
    data = [torch.tensor(tokenizer.encode(s), dtype=torch.long) for s in samples]
    #å¡«å……å¹¶å½¢æˆè¾“å…¥è¾“å‡ºå¯¹
    def collate_fn(batch, pad_id=0):
        batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=pad_id)
        return batch[:, :-1], batch[:, 1:]

    train_data, val_data, test_data = prepare_datasets_by_sequences(data)

    train_loader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_data, batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_data, batch_size, shuffle=False, collate_fn=collate_fn)
    model = TransformerEncoder(vocab_size=tokenizer.vocab_size)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model_path = "../results/best_model.pt"
    if os.path.exists(model_path):
        print("ğŸ“¥ åŠ è½½ä¹‹å‰è®­ç»ƒçš„æœ€ä½³æ¨¡å‹...")
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    else:
        print("ğŸ†• æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    optimizer = optim.AdamW(model.parameters(), lr=float(learning_rate),weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒå¾ªç¯ï¼ˆåŒ…å«éªŒè¯ï¼‰
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                out = model(x)
                loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch + 1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}")

        # æ—©åœæœºåˆ¶
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "../results/best_model.pt")
            print("  ä¿å­˜æœ€ä½³æ¨¡å‹ï¼")

    # æœ€ç»ˆæµ‹è¯•
    model.load_state_dict(torch.load("../results/best_model.pt"))
    test_loss = evaluate_model(model, test_loader, criterion, device)
    print(f"æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_loss:.4f}")

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plot_losses(train_losses, val_losses)


def evaluate_model_comprehensive(model, loader, criterion, device, tokenizer):
    """ç»¼åˆè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    correct_tokens = 0
    correct_top5 = 0

    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(loader):
            x, y = x.to(device), y.to(device)

            out = model(x)
            loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))
            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            predictions = out.argmax(dim=-1)

            # ç°åœ¨pad_token_idåº”è¯¥æœ‰æ•ˆäº†
            mask = (y != tokenizer.pad_token_id)
            print(f"Debug: mask shape: {mask.shape}")  # ç°åœ¨åº”è¯¥æ­£å¸¸äº†

            correct_tokens += ((predictions == y) & mask).sum().item()
            total_tokens += mask.sum().item()

            # è®¡ç®—Top-5å‡†ç¡®ç‡
            top5_pred = out.topk(5, dim=-1).indices
            top5_correct = torch.any(top5_pred == y.unsqueeze(-1), dim=-1)
            correct_top5 += (top5_correct & mask).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0
    top5_accuracy = correct_top5 / total_tokens if total_tokens > 0 else 0
    perplexity = torch.exp(torch.tensor(avg_loss)).item()

    return {
        'loss': avg_loss,
        'perplexity': perplexity,
        'accuracy': accuracy,
        'top5_accuracy': top5_accuracy,
        'total_tokens': total_tokens
    }

def generate_text_simple(model, tokenizer, device, prompt_tokens, max_length=50):
    """ç®€åŒ–ç‰ˆæ–‡æœ¬ç”Ÿæˆ"""
    model.eval()
    generated = prompt_tokens.clone().to(device)

    with torch.no_grad():
        for _ in range(max_length):
            if generated.size(1) >= 512:  # ä¸è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦
                break

            outputs = model(generated)
            next_token_logits = outputs[:, -1, :]
            next_token = next_token_logits.argmax(dim=-1, keepdim=True)

            # å¦‚æœç”Ÿæˆäº†ç»“æŸç¬¦åˆ™åœæ­¢
            if next_token.item() == tokenizer.eos_token_id:
                break

            generated = torch.cat([generated, next_token], dim=1)

    # è§£ç æ•´ä¸ªç”Ÿæˆåºåˆ—ï¼ˆåŒ…å«åŸå§‹æç¤ºï¼‰
    full_text = tokenizer.decode(generated[0].cpu().numpy(), skip_special_tokens=True)
    return full_text


def evaluate_with_text_comparison(model, loader, criterion, device, tokenizer):
    """è¯„ä¼°å¹¶åŒ…å«æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”"""
    # å…ˆè¿›è¡Œæ•°å€¼è¯„ä¼°
    metrics = evaluate_model_comprehensive(model, loader, criterion, device, tokenizer)

    # æ·»åŠ æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”
    model.eval()
    with torch.no_grad():
        # å–ç¬¬ä¸€ä¸ªbatchçš„ç¬¬ä¸€ä¸ªæ ·æœ¬
        for x, y in loader:
            x, y = x.to(device), y.to(device)

            # åŸå§‹æ–‡æœ¬
            original_tokens = x[0]
            # å»æ‰padding
            non_padding_tokens = original_tokens[original_tokens != tokenizer.pad_token_id]
            if len(non_padding_tokens) > 10:
                original_text = tokenizer.decode(non_padding_tokens.cpu().numpy())

                # ä½¿ç”¨å‰10ä¸ªtokenä½œä¸ºæç¤ºç”Ÿæˆæ–‡æœ¬
                prompt_tokens = non_padding_tokens[:10]
                prompt_text = tokenizer.decode(prompt_tokens.cpu().numpy())

                # æ”¹è¿›çš„ç”Ÿæˆï¼šä½¿ç”¨æ¸©åº¦è°ƒèŠ‚å’Œtop-ké‡‡æ ·
                generated_tokens = prompt_tokens.unsqueeze(0).to(device)
                for _ in range(50):  # ç”Ÿæˆ50ä¸ªtoken
                    outputs = model(generated_tokens)
                    next_token_logits = outputs[:, -1, :]

                    # æ¸©åº¦è°ƒèŠ‚å’Œtop-ké‡‡æ ·
                    next_token_logits = next_token_logits / 0.8  # æ¸©åº¦=0.8
                    top_k = 40
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = -float('Inf')
                    import torch.nn.functional as F
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)

                    generated_tokens = torch.cat([generated_tokens, next_token], dim=1)

                    if next_token.item() == tokenizer.eos_token_id:
                        break

                generated_text = tokenizer.decode(generated_tokens[0].cpu().numpy())

                metrics['text_comparison'] = {
                    'prompt': prompt_text,
                    'original': original_text[:150] + '...' if len(original_text) > 150 else original_text,
                    'generated': generated_text
                }
            break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬

    return metrics

if __name__ == "__main__":
    #ä½¿ç”¨ç¤ºä¾‹
    # text = get_tiny_shakespeare()
    # from transformers import GPT2TokenizerFast
    # tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    # if tokenizer.pad_token is None:
    #     tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨eos_tokenä½œä¸ºpad_token
    # print(f"Debug: pad_token_id after fix: {tokenizer.pad_token_id}")
    # # æŠŠæ–‡æœ¬åˆ‡å—æˆå°å¥å­
    # def split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32):
    #     """æŒ‰tokenè¾¹ç•Œåˆ‡å—ï¼Œé¿å…åˆ‡æ–­å•è¯"""
    #     # å…ˆtokenizeæ•´ä¸ªæ–‡æœ¬
    #     tokens = tokenizer.encode(text)
    #
    #     chunks = []
    #     start = 0
    #     while start < len(tokens):
    #         # å–ä¸€ä¸ªå—
    #         end = min(start + max_tokens, len(tokens))
    #         chunk_tokens = tokens[start:end]
    #
    #         # è§£ç å›æ–‡æœ¬ï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
    #         chunk_text = tokenizer.decode(chunk_tokens)
    #         chunks.append(chunk_text)
    #
    #         # æ»‘åŠ¨çª—å£ï¼Œä½¿ç”¨é‡å 
    #         start += (max_tokens - overlap)
    #
    #     return chunks
    # # ä½¿ç”¨æ”¹è¿›çš„åˆ‡å—
    # samples = split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32)
    # # åˆ†è¯
    # data = [torch.tensor(tokenizer.encode(s), dtype=torch.long) for s in samples]
    #
    #
    # # å¡«å……å¹¶å½¢æˆè¾“å…¥è¾“å‡ºå¯¹
    # def collate_fn(batch, tokenizer):
    #     """ä½¿ç”¨tokenizerçš„pad_token_id"""
    #     batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)
    #     return batch[:, :-1], batch[:, 1:]
    #
    # train_data, val_data, test_data = prepare_datasets_by_sequences(data)
    # criterion = nn.CrossEntropyLoss()
    # batch_size=32
    # train_loader = DataLoader(train_data, batch_size, shuffle=True,
    #                           collate_fn=lambda b: collate_fn(b, tokenizer))
    # val_loader = DataLoader(val_data, batch_size, shuffle=False,
    #                         collate_fn=lambda b: collate_fn(b, tokenizer))
    # test_loader = DataLoader(test_data, batch_size, shuffle=False,
    #                          collate_fn=lambda b: collate_fn(b, tokenizer))
    # model = TransformerEncoder(vocab_size=tokenizer.vocab_size)
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # model.to(device)
    # model.load_state_dict(torch.load("../results/best_model.pt"))
    # metrics = evaluate_with_text_comparison(model, test_loader, criterion, device, tokenizer)
    #
    # # æ‰“å°ç»“æœ
    # print(f"æœ€ç»ˆæµ‹è¯•æŸå¤±: {metrics['loss']:.4f}")
    # print(f"å›°æƒ‘åº¦: {metrics['perplexity']:.2f}")
    # print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    # print(f"Top-5å‡†ç¡®ç‡: {metrics['top5_accuracy']:.4f}")
    main()