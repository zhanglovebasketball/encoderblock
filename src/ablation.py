import matplotlib
matplotlib.use('Agg')
import os, torch, matplotlib.pyplot as plt
from model import TransformerEncoder
from torch.utils.data import DataLoader
import torch.nn as nn, torch.optim as optim
from transformers import GPT2TokenizerFast
import random, numpy as np, requests
from utils.config import load_config
def prepare_datasets_by_sequences(data, train_ratio=0.8, val_ratio=0.1,seed=42):
    """æŒ‰åºåˆ—åˆ†å‰²ï¼Œé¿å…åˆ‡æ–­ä¸Šä¸‹æ–‡"""
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    random.seed(seed)
    # å…ˆæ‰“ä¹±æ•°æ®é¡ºåº
    shuffled_data = data.copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
    random.shuffle(shuffled_data)
    total_sequences = len(shuffled_data)
    train_size = int(total_sequences * train_ratio)
    val_size = int(total_sequences * val_ratio)

    train_data = shuffled_data[:train_size]
    val_data = shuffled_data[train_size:train_size + val_size]
    test_data = shuffled_data[train_size + val_size:]

    return train_data, val_data, test_data

def evaluate_model(model, loader, criterion, device):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))
            total_loss += loss.item()
    return total_loss / len(loader)
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def get_tiny_shakespeare():
    os.makedirs("data", exist_ok=True)
    path = "../data/tiny_shakespeare.txt"
    if not os.path.exists(path):
        print("ğŸ“¥ ä¸‹è½½ tiny_shakespeare.txt ...")
        url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        r = requests.get(url)
        with open(path, "w", encoding="utf-8") as f:
            f.write(r.text)
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    return text[:400000]

def collate_fn(batch, pad_id=0):
    batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=pad_id)
    return batch[:, :-1], batch[:, 1:]

def run_experiment(use_positional=True, num_heads=4, label="base"):
    # åŠ è½½é…ç½®
    cfg = load_config()
    batch_size = cfg['batch_size']
    learning_rate = cfg['learning_rate']
    set_seed(42)
    text = get_tiny_shakespeare()
    from transformers import GPT2TokenizerFast
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

    # æŠŠæ–‡æœ¬åˆ‡å—æˆå°å¥å­
    def split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32):
        """æŒ‰tokenè¾¹ç•Œåˆ‡å—ï¼Œé¿å…åˆ‡æ–­å•è¯"""
        # å…ˆtokenizeæ•´ä¸ªæ–‡æœ¬
        tokens = tokenizer.encode(text)

        chunks = []
        start = 0
        while start < len(tokens):
            # å–ä¸€ä¸ªå—
            end = min(start + max_tokens, len(tokens))
            chunk_tokens = tokens[start:end]

            # è§£ç å›æ–‡æœ¬ï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
            chunk_text = tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)

            # æ»‘åŠ¨çª—å£ï¼Œä½¿ç”¨é‡å 
            start += (max_tokens - overlap)

        return chunks

    # ä½¿ç”¨æ”¹è¿›çš„åˆ‡å—
    samples = split_text_by_tokens(text, tokenizer, max_tokens=128, overlap=32)
    # åˆ†è¯
    data = [torch.tensor(tokenizer.encode(s), dtype=torch.long) for s in samples]

    # å¡«å……å¹¶å½¢æˆè¾“å…¥è¾“å‡ºå¯¹
    def collate_fn(batch, pad_id=0):
        batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=pad_id)
        return batch[:, :-1], batch[:, 1:]

    train_data, val_data, test_data = prepare_datasets_by_sequences(data)

    train_loader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_data, batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_data, batch_size, shuffle=False, collate_fn=collate_fn)

    model = TransformerEncoder(vocab_size=tokenizer.vocab_size, num_heads=num_heads)
    if not use_positional:
        model.pos = nn.Identity()  # ç§»é™¤ä½ç½®ç¼–ç 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = optim.AdamW(model.parameters(), lr=float(learning_rate))
    criterion = nn.CrossEntropyLoss()
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    epochs=60
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                out = model(x)
                loss = criterion(out.view(-1, out.size(-1)), y.reshape(-1))
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch + 1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}")

        # æ—©åœæœºåˆ¶
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "../results/best_model_ablation.pt")
            print("  ä¿å­˜æœ€ä½³æ¨¡å‹ï¼")

    # æœ€ç»ˆæµ‹è¯•
    model.load_state_dict(torch.load("../results/best_model_ablation.pt"))
    test_loss = evaluate_model(model, test_loader, criterion, device)
    print(f"æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_loss:.4f}")

    return train_losses

def main():
    os.makedirs("../results", exist_ok=True)
    runs = {
        "no_pos": run_experiment(use_positional=False, label="no_pos"),
    }

    plt.figure()
    for k, v in runs.items():
        plt.plot(v, label=k)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Ablation Study: Effect of Position Encoding and #Heads")
    plt.legend()
    plt.savefig("../results/ablation.png")


if __name__ == "__main__":
    main()



